{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basics \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "# PySpark Streaming\n",
    "from pyspark.streaming import StreamingContext\n",
    "from threading import Thread\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import rand, udf \n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "\n",
    "#NLTK\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# PySPark MLLib\n",
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import  Tokenizer\n",
    "from pyspark.ml.feature import  IDF, IDFModel\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.feature import IndexToString\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "class StreamingThread(Thread):\n",
    "    def __init__(self, ssc):\n",
    "        Thread.__init__(self)\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        ssc.start()\n",
    "        ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read streamed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"tweet_id\": 1384072187976450054, \"tweet_text\": \"@theage Can\\\\u2019t threaten a #\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588 program that doesn\\\\u2019t exist. #\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\", \"label\": \"#covid\"}',\n",
       " '{\"tweet_id\": 1384072149002952706, \"tweet_text\": \"#\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588 or #\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588 Time People thought Corona Vanished started Holding Gatherings ! \\\\nInstead of utilising time to prepare for all the Panic Today we are facing !! #\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588 of Everything\\\\n#\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588 #\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588 is not Enough !\\\\n#\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588\\\\u2588 \\\\n@PMOIndia \\\\ud83c\\\\uddee\\\\ud83c\\\\uddf3\\\\n@ArvindKejriwal \\\\n@rashtrapatibhvn\", \"label\": \"#covid\"}']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tryout reading invidivual files into RDD format \n",
    "\n",
    "rdd2 = spark.sparkContext.textFile(\"spark/tweets_streamed/tweet-1618824030000\")\n",
    "test = rdd2.collect()\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read streamed tweets into RDD then Python List\n",
    "\n",
    "counter = 0\n",
    "tweets = []\n",
    "saved_texts_list = os.listdir(\"Desktop/spark/tweets_streamed/\")\n",
    "for text_partition in saved_texts_list:\n",
    "    rdd = spark.sparkContext.textFile(\"Desktop/spark/tweets_streamed/\" + text_partition)\n",
    "    for tweet in rdd.collect():\n",
    "        tweets.append(tweet)\n",
    "    if (counter % 100 == 0):\n",
    "        print(str(counter).ljust(5), 'files processed!')\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#vaccine          432\n",
      "#covid            392\n",
      "#china            326\n",
      "#biden            117\n",
      "#stopasianhate     95\n",
      "#inflation         52\n",
      "Name: label, dtype: int64\n",
      "1414 1456\n"
     ]
    }
   ],
   "source": [
    "# Check - Total streamed tweets and value counts\n",
    "\n",
    "tweet_ids = []\n",
    "tweet_texts = []\n",
    "tweet_labels = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweet_dict = eval(tweet)\n",
    "    tweet_ids.append(tweet_dict['tweet_id'])\n",
    "    tweet_texts.append(tweet_dict['tweet_text'])\n",
    "    tweet_labels.append(tweet_dict['label'])\n",
    "\n",
    "tweet_df = pd.DataFrame({'id': tweet_ids, 'text': tweet_texts, 'label': tweet_labels})\n",
    "tweet_df_filtered = tweet_df.drop_duplicates(['id'])\n",
    "print(tweet_df_filtered.label.value_counts())\n",
    "print(len(tweet_df_filtered), len(tweet_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save as a CSV file\n",
    "\n",
    "tweet_df_filtered[\"text\"] = tweet_df_filtered[\"text\"].str.encode('utf-8', 'ignore').str.decode('utf-8')\n",
    "tweet_df_filtered.to_csv(\"Desktop/spark/tweets.csv\", index = False, sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1384071564216340485</td>\n",
       "      <td>#███████: People gather in large numbers outsi...</td>\n",
       "      <td>#vaccine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1384072059060326401</td>\n",
       "      <td>But for sure, #███████ is not a solution, we n...</td>\n",
       "      <td>#covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1384072056233361408</td>\n",
       "      <td>We have webinars from @VitaSafety with lots of...</td>\n",
       "      <td>#covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1384072031465984005</td>\n",
       "      <td>For anyone who has #███████ and is isolating a...</td>\n",
       "      <td>#covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1384071995726262276</td>\n",
       "      <td>@ndtv These animals should be put behind bars ...</td>\n",
       "      <td>#covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>1384206019362922510</td>\n",
       "      <td>Human Rights Watch Calls Out #███████’s ‘Crime...</td>\n",
       "      <td>#china</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410</th>\n",
       "      <td>1384206642586157060</td>\n",
       "      <td>#███████ talk say #███████ leak from their lab...</td>\n",
       "      <td>#china</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411</th>\n",
       "      <td>1384206577410797568</td>\n",
       "      <td>#███████ loudly tout liberal values &amp;amp; mino...</td>\n",
       "      <td>#china</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>1384207020694269961</td>\n",
       "      <td>FRESH PORTSMOUTH CAREER MODE!! ZOMBIES FEAT.MU...</td>\n",
       "      <td>#stopasianhate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>1384207097521311758</td>\n",
       "      <td>What to do if you hear a racist and Anti-Asian...</td>\n",
       "      <td>#stopasianhate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1414 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                               text  \\\n",
       "0     1384071564216340485  #███████: People gather in large numbers outsi...   \n",
       "1     1384072059060326401  But for sure, #███████ is not a solution, we n...   \n",
       "2     1384072056233361408  We have webinars from @VitaSafety with lots of...   \n",
       "3     1384072031465984005  For anyone who has #███████ and is isolating a...   \n",
       "4     1384071995726262276  @ndtv These animals should be put behind bars ...   \n",
       "...                   ...                                                ...   \n",
       "1409  1384206019362922510  Human Rights Watch Calls Out #███████’s ‘Crime...   \n",
       "1410  1384206642586157060  #███████ talk say #███████ leak from their lab...   \n",
       "1411  1384206577410797568  #███████ loudly tout liberal values &amp; mino...   \n",
       "1412  1384207020694269961  FRESH PORTSMOUTH CAREER MODE!! ZOMBIES FEAT.MU...   \n",
       "1413  1384207097521311758  What to do if you hear a racist and Anti-Asian...   \n",
       "\n",
       "               label  \n",
       "0           #vaccine  \n",
       "1             #covid  \n",
       "2             #covid  \n",
       "3             #covid  \n",
       "4             #covid  \n",
       "...              ...  \n",
       "1409          #china  \n",
       "1410          #china  \n",
       "1411          #china  \n",
       "1412  #stopasianhate  \n",
       "1413  #stopasianhate  \n",
       "\n",
       "[1414 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To read from the CSV file\n",
    "\n",
    "tweet_df_filtered = pd.read_csv(\"Desktop/spark/tweets.csv\", sep=';')\n",
    "tweet_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#vaccine          432\n",
      "#covid            392\n",
      "#china            326\n",
      "#biden            117\n",
      "#stopasianhate     95\n",
      "#inflation         52\n",
      "Name: label, dtype: int64\n",
      "1414\n"
     ]
    }
   ],
   "source": [
    "print(tweet_df_filtered.label.value_counts())\n",
    "print(len(tweet_df_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1384845659136831490</td>\n",
       "      <td>Chauvin's murder conviction makes history in A...</td>\n",
       "      <td>#biden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1384903605132607492</td>\n",
       "      <td>#███████ Apology Lacks ‘Sincerity’,  #███████’...</td>\n",
       "      <td>#china</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1384904382580469761</td>\n",
       "      <td>If Biden can ban words like \"Illegal\" \"Illegal...</td>\n",
       "      <td>#biden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1384903264630714372</td>\n",
       "      <td>Maharashtra CM likely to impose lockdown today...</td>\n",
       "      <td>#covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1384873158373691392</td>\n",
       "      <td>In the #███████, #███████ has more than twice ...</td>\n",
       "      <td>#vaccine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>1384903106123739136</td>\n",
       "      <td>There is no relevance of #███████ with #██████...</td>\n",
       "      <td>#inflation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>1384874658030366724</td>\n",
       "      <td>EMBARRASSING Ontario is reporting 4212 cases o...</td>\n",
       "      <td>#vaccine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>1384874577801752578</td>\n",
       "      <td>The Covid-19 pandemic has had a devastating im...</td>\n",
       "      <td>#vaccine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>1384852038597681153</td>\n",
       "      <td>#███████ invested $350M in Turkish #███████ pl...</td>\n",
       "      <td>#china</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>1384852758948425730</td>\n",
       "      <td>Check out ..... Loading .....'s video! #██████...</td>\n",
       "      <td>#vaccine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1190 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                               text  \\\n",
       "0     1384845659136831490  Chauvin's murder conviction makes history in A...   \n",
       "1     1384903605132607492  #███████ Apology Lacks ‘Sincerity’,  #███████’...   \n",
       "2     1384904382580469761  If Biden can ban words like \"Illegal\" \"Illegal...   \n",
       "3     1384903264630714372  Maharashtra CM likely to impose lockdown today...   \n",
       "4     1384873158373691392  In the #███████, #███████ has more than twice ...   \n",
       "...                   ...                                                ...   \n",
       "1185  1384903106123739136  There is no relevance of #███████ with #██████...   \n",
       "1186  1384874658030366724  EMBARRASSING Ontario is reporting 4212 cases o...   \n",
       "1187  1384874577801752578  The Covid-19 pandemic has had a devastating im...   \n",
       "1188  1384852038597681153  #███████ invested $350M in Turkish #███████ pl...   \n",
       "1189  1384852758948425730  Check out ..... Loading .....'s video! #██████...   \n",
       "\n",
       "           label  \n",
       "0         #biden  \n",
       "1         #china  \n",
       "2         #biden  \n",
       "3         #covid  \n",
       "4       #vaccine  \n",
       "...          ...  \n",
       "1185  #inflation  \n",
       "1186    #vaccine  \n",
       "1187    #vaccine  \n",
       "1188      #china  \n",
       "1189    #vaccine  \n",
       "\n",
       "[1190 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To read from the CSV file - Nina (Merge with groupmates' stream)\n",
    "\n",
    "tweet_df_filtered_nina = pd.read_csv(\"Desktop/spark/tweets_nina.csv\", sep=';', lineterminator='\\n')\n",
    "tweet_df_filtered_nina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1384071564216340485</td>\n",
       "      <td>#███████: People gather in large numbers outsi...</td>\n",
       "      <td>#vaccine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1384072059060326401</td>\n",
       "      <td>But for sure, #███████ is not a solution, we n...</td>\n",
       "      <td>#covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1384072056233361408</td>\n",
       "      <td>We have webinars from @VitaSafety with lots of...</td>\n",
       "      <td>#covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1384072031465984005</td>\n",
       "      <td>For anyone who has #███████ and is isolating a...</td>\n",
       "      <td>#covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1384071995726262276</td>\n",
       "      <td>@ndtv These animals should be put behind bars ...</td>\n",
       "      <td>#covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>1384903106123739136</td>\n",
       "      <td>There is no relevance of #███████ with #██████...</td>\n",
       "      <td>#inflation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>1384874658030366724</td>\n",
       "      <td>EMBARRASSING Ontario is reporting 4212 cases o...</td>\n",
       "      <td>#vaccine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>1384874577801752578</td>\n",
       "      <td>The Covid-19 pandemic has had a devastating im...</td>\n",
       "      <td>#vaccine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>1384852038597681153</td>\n",
       "      <td>#███████ invested $350M in Turkish #███████ pl...</td>\n",
       "      <td>#china</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>1384852758948425730</td>\n",
       "      <td>Check out ..... Loading .....'s video! #██████...</td>\n",
       "      <td>#vaccine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2604 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                               text  \\\n",
       "0     1384071564216340485  #███████: People gather in large numbers outsi...   \n",
       "1     1384072059060326401  But for sure, #███████ is not a solution, we n...   \n",
       "2     1384072056233361408  We have webinars from @VitaSafety with lots of...   \n",
       "3     1384072031465984005  For anyone who has #███████ and is isolating a...   \n",
       "4     1384071995726262276  @ndtv These animals should be put behind bars ...   \n",
       "...                   ...                                                ...   \n",
       "1185  1384903106123739136  There is no relevance of #███████ with #██████...   \n",
       "1186  1384874658030366724  EMBARRASSING Ontario is reporting 4212 cases o...   \n",
       "1187  1384874577801752578  The Covid-19 pandemic has had a devastating im...   \n",
       "1188  1384852038597681153  #███████ invested $350M in Turkish #███████ pl...   \n",
       "1189  1384852758948425730  Check out ..... Loading .....'s video! #██████...   \n",
       "\n",
       "           label  \n",
       "0       #vaccine  \n",
       "1         #covid  \n",
       "2         #covid  \n",
       "3         #covid  \n",
       "4         #covid  \n",
       "...          ...  \n",
       "1185  #inflation  \n",
       "1186    #vaccine  \n",
       "1187    #vaccine  \n",
       "1188      #china  \n",
       "1189    #vaccine  \n",
       "\n",
       "[2604 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merge frames\n",
    "\n",
    "tweet_df_filtered_merged = pd.concat([tweet_df_filtered, tweet_df_filtered_nina])\n",
    "tweet_df_filtered_merged = tweet_df_filtered_merged.drop_duplicates(['id'])\n",
    "tweet_df_filtered_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV file\n",
    "\n",
    "tweet_df_filtered_merged[\"text\"] = tweet_df_filtered_merged[\"text\"].str.encode('utf-8', 'ignore').str.decode('utf-8')\n",
    "tweet_df_filtered_merged.to_csv(\"Desktop/spark/tweets_merged.csv\", index = False, sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load merged CSV tweets\n",
    "\n",
    "tweet_df_filtered_merged = pd.read_csv(\"spark/notebooks/tweets_merged.csv\", sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#vaccine          744\n",
      "#covid            672\n",
      "#china            649\n",
      "#biden            255\n",
      "#stopasianhate    184\n",
      "#inflation        100\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Final hashtag (label) value counts - Total 2604 tweets streamed\n",
    "\n",
    "tweet_df_filtered = tweet_df_filtered_merged\n",
    "print(tweet_df_filtered.label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text pre-processing (remove punctuation, blocks and trim spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text by substitutions\n",
    "\n",
    "def remove_punct(text):\n",
    "    #clean  new line\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    #clean full blocks\n",
    "    text = re.sub('\\u2588', '', text)\n",
    "    #number of #hashtags\n",
    "    text = text + ' ' + str(text.count('#'))\n",
    "    #clean #\n",
    "    text = re.sub('#', '', text)\n",
    "    #Clean links\n",
    "    text = re.sub('http.* ', ' urllink ', text)\n",
    "    #multispace to singlespace\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    #exp ozge's to ozges\n",
    "    text = re.sub('\\'', '', text)\n",
    "    text = re.sub('\\’', '', text)\n",
    "    #create a space for punctuation\n",
    "    text = re.sub('[!\"$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~“”]', ' ', text)\n",
    "    #multispace to singlespace\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text.lower()\n",
    "\n",
    "tweet_df_filtered = tweet_df_filtered.reset_index(drop=True)\n",
    "tweet_df_filtered['cleaned_text'] = ''\n",
    "for i in range(len(tweet_df_filtered)):\n",
    "    tweet_df_filtered.loc[i, 'cleaned_text']= remove_punct(tweet_df_filtered['text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“WE WILL NOT BE INTIMIDATED.” DESPITE CHINA’S THREATS, LITHUANIA MOVES TO RECOGNISE UIGHUR GENOCIDENEW\\n\\nhttps://t.co/hLjIGhBFST\\n#███████ #███████ #███████ #███████ #███████ #███████ #███████\\n#███████ https://t.co/gDocjvTSof'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of a raw tweet \n",
    "\n",
    "tweet_df_filtered.loc[1500]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' we will not be intimidated despite chinas threats lithuania moves to recognise uighur genocidenew urllink 8'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of a the cleaned vrsion of the above tweet\n",
    "# We additionally include hasgtag count as an integer so that document vector transformations can notice\n",
    "# Additionally, we swich each URL with a unique keyword 'urllink' for text # Example of a raw tweet \n",
    "\n",
    "tweet_df_filtered.loc[1500]['cleaned_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training with MLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare data to create a PySpark dataframe\n",
    "\n",
    "spark_tuples = []\n",
    "for i in range(len(tweet_df_filtered)):\n",
    "    spark_tuples.append((tweet_df_filtered['label'][i], tweet_df_filtered['cleaned_text'][i]))  \n",
    "    \n",
    "#Create PySpark Dataframe\n",
    "\n",
    "tweet_data = spark.createDataFrame(spark_tuples, [\"label\", \"tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|   label|               tweet|\n",
      "+--------+--------------------+\n",
      "|#vaccine| people gather in...|\n",
      "|  #covid|but for sure is n...|\n",
      "|  #covid|we have webinars ...|\n",
      "|  #covid|for anyone who ha...|\n",
      "|  #covid| ndtv these anima...|\n",
      "|  #covid|starting the week...|\n",
      "|  #covid| theage cant thre...|\n",
      "|  #covid| or time people t...|\n",
      "|  #covid|addressing the si...|\n",
      "|  #china|the chinese offic...|\n",
      "+--------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweet_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|         label|count|\n",
      "+--------------+-----+\n",
      "|        #biden|  734|\n",
      "|      #vaccine|  744|\n",
      "|        #china|  735|\n",
      "|        #covid|  734|\n",
      "|#stopasianhate|  725|\n",
      "|    #inflation|  710|\n",
      "+--------------+-----+\n",
      "\n",
      "+--------------+--------------------+\n",
      "|         label|               tweet|\n",
      "+--------------+--------------------+\n",
      "|        #covid|district overview...|\n",
      "|        #biden|           urllink 5|\n",
      "|        #china|we can see why th...|\n",
      "|    #inflation| official central...|\n",
      "|        #china| arabia firmly su...|\n",
      "|        #china|a shenzhen compan...|\n",
      "|        #china| loudly tout libe...|\n",
      "|      #vaccine|thank you drnigha...|\n",
      "|#stopasianhate| hcphtx readyharr...|\n",
      "|      #vaccine|as of today every...|\n",
      "+--------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Oversampling\n",
    "\n",
    "highest_label_count = tweet_data.where(tweet_data['label']=='#vaccine').count()\n",
    "covid_oversampled = tweet_data.where(tweet_data['label']=='#covid').sample(True, (highest_label_count/(tweet_data.where(tweet_data['label']=='#covid').count())-1), seed = 123)\n",
    "china_oversampled = tweet_data.where(tweet_data['label']=='#china').sample(True, (highest_label_count/(tweet_data.where(tweet_data['label']=='#china').count())-1), seed = 123)\n",
    "biden_oversampled = tweet_data.where(tweet_data['label']=='#biden').sample(True, (highest_label_count/(tweet_data.where(tweet_data['label']=='#biden').count())-1), seed = 123)\n",
    "stopasianhate_oversampled = tweet_data.where(tweet_data['label']=='#stopasianhate').sample(True, (highest_label_count/(tweet_data.where(tweet_data['label']=='#stopasianhate').count())-1), seed = 123)\n",
    "inflation_oversampled = tweet_data.where(tweet_data['label']=='#inflation').sample(True, (highest_label_count/(tweet_data.where(tweet_data['label']=='#inflation').count())-1), seed = 123)\n",
    "\n",
    "# Merged oversampled tweets\n",
    "\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "oversampled_df = unionAll(covid_oversampled, china_oversampled, biden_oversampled, stopasianhate_oversampled, inflation_oversampled, tweet_data)\n",
    "oversampled_df.groupBy('label').count().show()\n",
    "oversampled_df = oversampled_df.orderBy(rand())\n",
    "oversampled_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+\n",
      "|   label|               tweet|               words|       cleaned_words|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "|#vaccine| people gather in...|[, people, gather...|[, peopl, gather,...|\n",
      "|  #covid|but for sure is n...|[but, for, sure, ...|[sure, solut, nee...|\n",
      "|  #covid|we have webinars ...|[we, have, webina...|[webinar, vitasaf...|\n",
      "|  #covid|for anyone who ha...|[for, anyone, who...|[anyon, isol, hom...|\n",
      "|  #covid| ndtv these anima...|[, ndtv, these, a...|[, ndtv, anim, pu...|\n",
      "+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------+--------------------+\n",
      "|         label|            features|\n",
      "+--------------+--------------------+\n",
      "|      #vaccine|(200,[0,1,12,62,1...|\n",
      "|        #covid|(200,[4,25,70],[1...|\n",
      "|        #covid|(200,[0,2,123,164...|\n",
      "|        #covid|(200,[0,3,7,101,1...|\n",
      "|        #covid|(200,[1,2],[1.144...|\n",
      "|        #covid|(200,[0,4,34,54,1...|\n",
      "|        #covid|(200,[1,2,106],[1...|\n",
      "|        #covid|(200,[1,12,13,30,...|\n",
      "|        #covid|(200,[8,25,28,30,...|\n",
      "|        #china|(200,[0,6,38,56],...|\n",
      "|        #china|(200,[4,54,105,11...|\n",
      "|        #china|(200,[0,2,14,27,7...|\n",
      "|        #covid|(200,[0,4,9,26,37...|\n",
      "|#stopasianhate|(200,[0,22,31,67,...|\n",
      "|        #covid|(200,[1,4,9,14,16...|\n",
      "|        #biden|(200,[0,1,4,57,71...|\n",
      "|        #biden|(200,[0,2,79,80],...|\n",
      "|        #china|(200,[0,5,11,13,1...|\n",
      "|        #china|(200,[0,6],[0.329...|\n",
      "|        #china|(200,[0,4,14,15,2...|\n",
      "+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "#Count vectorizer + idf approach - Transformation#\n",
    "##################################################\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(tweet_data)\n",
    "\n",
    "#Removing stop words\n",
    "remover = StopWordsRemover(inputCol = \"words\", outputCol = \"cleaned_words\")\n",
    "df_cleaned_text = remover.transform(wordsData)\n",
    "\n",
    "#Stemming\n",
    "stemmer = SnowballStemmer(language = \"english\")\n",
    "stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "df_stemmed = df_cleaned_text.withColumn(\"cleaned_words\", stemmer_udf(\"cleaned_words\"))\n",
    "df_stemmed.show(5)\n",
    "\n",
    "#Term frequency-TF\n",
    "count = CountVectorizer(inputCol=\"cleaned_words\", outputCol=\"rawFeatures\", vocabSize=200)\n",
    "count_vectorizer_model = count.fit(df_stemmed)\n",
    "featurizedData = count_vectorizer_model.transform(df_stemmed)\n",
    "\n",
    "#IDF \n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "#Showing\n",
    "rescaledData.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|         label|            features|\n",
      "+--------------+--------------------+\n",
      "|    #inflation|(50,[1,3,4,6,12,1...|\n",
      "|#stopasianhate|(50,[2,6,7,13,15,...|\n",
      "|        #covid|(50,[15,17,18,42,...|\n",
      "|    #inflation|(50,[4,6,13,18,23...|\n",
      "|        #china|(50,[6,16,21,22,2...|\n",
      "|#stopasianhate|(50,[1,5,6,7,9,10...|\n",
      "|        #china|(50,[5,10,11,14,1...|\n",
      "|    #inflation|(50,[18,20,22,24]...|\n",
      "|      #vaccine|(50,[1,3,6,10,15,...|\n",
      "|        #covid|(50,[1,3,5,6,10,1...|\n",
      "|        #biden|(50,[0,7,30,41,42...|\n",
      "|      #vaccine|(50,[6,9,10,11,13...|\n",
      "|        #biden|(50,[0,2,3,4,6,7,...|\n",
      "|      #vaccine|(50,[3,4,5,7,11,1...|\n",
      "|        #biden|(50,[1,2,5,9,11,1...|\n",
      "|        #covid|(50,[0,4,9,12,17,...|\n",
      "|#stopasianhate|(50,[0,1,3,6,7,8,...|\n",
      "|    #inflation|(50,[6,10,11,14,1...|\n",
      "|      #vaccine|(50,[3,4,5,6,7,17...|\n",
      "|        #covid|(50,[0,2,5,6,11,1...|\n",
      "+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DO NOT USE - JUST EXPERIMENTAL\n",
    "############################################\n",
    "#Hashing TF + idf approach - Transformation#\n",
    "############################################\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(oversampled_df)\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=50)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "rescaledData.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Test Split \n",
    "# Fianlly, we train with all the data but used for model search during grid search\n",
    "\n",
    "(trainingData, testData) = rescaledData.randomSplit([1.0, 0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      "DenseMatrix([[ 0.7051162 , -0.17784361,  0.2248641 , ...,  0.43509169,\n",
      "               0.43435063, -0.20761375],\n",
      "             [ 0.11836464, -0.29468858,  0.26707827, ...,  0.27631729,\n",
      "               0.53671436, -0.29302385],\n",
      "             [ 1.90021664,  0.36287638, -0.04132572, ...,  0.3123655 ,\n",
      "               0.50760221, -0.13386779],\n",
      "             [ 0.92805262,  0.19079111, -0.70005112, ...,  0.15253045,\n",
      "              -1.29589792, -0.51441475],\n",
      "             [-2.3166265 , -0.32894427, -0.02704546, ..., -1.96633118,\n",
      "               0.7465178 ,  0.68618354],\n",
      "             [-1.33512361,  0.24780896,  0.27647994, ...,  0.79002626,\n",
      "              -0.92928709,  0.4627366 ]])\n",
      "Intercept: [0.4799018948267931,0.5119622494672291,0.3831667021129153,-0.010408835666568904,0.10695604917990899,-1.4715780599202777]\n"
     ]
    }
   ],
   "source": [
    "#Create a label indexer (string labels to double is required)\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(rescaledData)\n",
    "lr = LogisticRegression(maxIter=50, labelCol=\"indexedLabel\")\n",
    "pipeline = Pipeline(stages=[labelIndexer, lr])\n",
    "\n",
    "#Fit the pipeline to training documents\n",
    "\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "#Print the coefficients and intercept for multinomial logistic regression\n",
    "\n",
    "print(\"Coefficients: \\n\" + str(model.stages[-1].coefficientMatrix))\n",
    "print(\"Intercept: \" + str(model.stages[-1].interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the models for deployment\n",
    "\n",
    "#CountVectorizer\n",
    "count_vectorizer_model.save('count_vectorizer_model_prototype_5')\n",
    "\n",
    "#IDF\n",
    "idfModel.save('idf_model_prototype_5')\n",
    "\n",
    "#Indexed Labels\n",
    "indexed_df = labelIndexer.transform(trainingData)\n",
    "meta = [f.metadata for f in indexed_df.schema.fields if f.name == \"indexedLabel\"]\n",
    "index_dict = dict(enumerate(meta[0][\"ml_attr\"][\"vals\"]))\n",
    "\n",
    "index_dict['0.0'] = index_dict.pop(0)\n",
    "index_dict['1.0'] = index_dict.pop(1)\n",
    "index_dict['2.0'] = index_dict.pop(2)\n",
    "index_dict['3.0'] = index_dict.pop(3)\n",
    "index_dict['4.0'] = index_dict.pop(4)\n",
    "index_dict['5.0'] = index_dict.pop(5)\n",
    "\n",
    "with open('index_hashtag_dict.json', 'w') as fp:\n",
    "    json.dump(index_dict, fp)\n",
    "    \n",
    "#ML model\n",
    "\n",
    "model.stages[-1].save(\"logistic_regression_model_prototype_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(testData)\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RandomForest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a label indexer (string labels to double is required)\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(rescaledData)\n",
    "rf = RandomForestClassifier(labelCol = \"indexedLabel\", numTrees = 100)\n",
    "pipeline = Pipeline(stages=[labelIndexer, rf])\n",
    "\n",
    "#Fit the pipeline to training documents\n",
    "\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the models for deployment\n",
    "\n",
    "#CountVectorizer\n",
    "count_vectorizer_model.save('count_vectorizer_model_prototype_7')\n",
    "\n",
    "#IDF\n",
    "idfModel.save('idf_model_prototype_7')\n",
    "\n",
    "#Indexed Labels\n",
    "indexed_df = labelIndexer.transform(trainingData)\n",
    "meta = [f.metadata for f in indexed_df.schema.fields if f.name == \"indexedLabel\"]\n",
    "index_dict = dict(enumerate(meta[0][\"ml_attr\"][\"vals\"]))\n",
    "\n",
    "index_dict['0.0'] = index_dict.pop(0)\n",
    "index_dict['1.0'] = index_dict.pop(1)\n",
    "index_dict['2.0'] = index_dict.pop(2)\n",
    "index_dict['3.0'] = index_dict.pop(3)\n",
    "index_dict['4.0'] = index_dict.pop(4)\n",
    "index_dict['5.0'] = index_dict.pop(5)\n",
    "\n",
    "import json\n",
    "with open('index_hashtag_dict.json', 'w') as fp:\n",
    "    json.dump(index_dict, fp)\n",
    "    \n",
    "#ML model\n",
    "\n",
    "model.stages[-1].save(\"random_forest_model_prototype_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation and Parameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For cross-validation\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"newlabel\")\n",
    "model = stringIndexer.fit(rescaledData)\n",
    "df = model.transform(rescaledData)\n",
    "cv_set = df.selectExpr(\"features as features\", \"newlabel as label\")\n",
    "cv_set.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter = 20)\n",
    "paramGrid_lr = ParamGridBuilder().addGrid(lr.regParam, np.linspace(0.3, 0.01, 3)).addGrid(lr.elasticNetParam, np.linspace(0.3, 0.8, 6)).build()\n",
    "crossval_lr = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid_lr,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds= 5)  \n",
    "                \n",
    "cvModel_lr = crossval_lr.fit(cv_set)\n",
    "best_model_lr = cvModel_lr.bestModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Param (regParam):  0.01\n",
      "Best Param (elasticNetParam):  0.5\n"
     ]
    }
   ],
   "source": [
    "best_model_lr = cvModel_lr.bestModel\n",
    "print('Best Param (regParam): ', best_model_lr._java_obj.getRegParam())\n",
    "print('Best Param (elasticNetParam): ', best_model_lr._java_obj.getElasticNetParam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       0.0|         3.0|(50,[1,2,3,7,21,2...|\n",
      "|       2.0|         3.0|(50,[1,2,13,16,17...|\n",
      "|       0.0|         3.0|(50,[1,3,4,6,7,9,...|\n",
      "|       0.0|         3.0|(50,[1,2,4,6,8,10...|\n",
      "|       1.0|         3.0|(50,[0,1,2,5,9,13...|\n",
      "|       2.0|         3.0|(50,[0,1,6,13,21,...|\n",
      "|       1.0|         2.0|(50,[0,1,5,7,8,9,...|\n",
      "|       1.0|         2.0|(50,[1,8,12,13,15...|\n",
      "|       1.0|         2.0|(50,[1,4,6,11,13,...|\n",
      "|       1.0|         2.0|(50,[0,1,3,12,16,...|\n",
      "|       0.0|         2.0|(50,[0,1,3,5,6,7,...|\n",
      "|       0.0|         2.0|(50,[1,2,3,6,9,10...|\n",
      "|       0.0|         2.0|(50,[1,4,5,8,13,1...|\n",
      "|       2.0|         2.0|(50,[1,4,6,7,8,9,...|\n",
      "|       0.0|         2.0|(50,[0,1,2,17,20,...|\n",
      "|       0.0|         2.0|(50,[0,1,3,5,6,8,...|\n",
      "|       1.0|         2.0|(50,[0,1,2,6,9,10...|\n",
      "|       2.0|         2.0|(50,[1,3,4,5,14,2...|\n",
      "|       1.0|         2.0|(50,[1,5,7,16,17,...|\n",
      "|       2.0|         2.0|(50,[1,5,13,16,17...|\n",
      "|       1.0|         2.0|(50,[1,3,6,8,9,15...|\n",
      "|       2.0|         2.0|(50,[1,2,9,10,13,...|\n",
      "|       2.0|         2.0|(50,[1,2,3,4,7,9,...|\n",
      "|       1.0|         2.0|(50,[1,3,8,11,19,...|\n",
      "|       0.0|         1.0|(50,[0,1,2,6,9,10...|\n",
      "|       0.0|         1.0|(50,[0,1,2,3,5,6,...|\n",
      "|       1.0|         1.0|(50,[1,5,7,12,13,...|\n",
      "|       1.0|         1.0|(50,[0,1,4,5,9,10...|\n",
      "|       0.0|         1.0|(50,[1,6,8,10,14,...|\n",
      "|       1.0|         1.0|(50,[0,1,3,5,8,12...|\n",
      "|       0.0|         1.0|(50,[1,4,11,12,16...|\n",
      "|       1.0|         1.0|(50,[0,1,9,12,13,...|\n",
      "|       1.0|         5.0|(50,[0,1,3,5,6,7,...|\n",
      "|       0.0|         4.0|(50,[1,17,22,30,3...|\n",
      "|       1.0|         4.0|(50,[0,1,9,10,13,...|\n",
      "|       0.0|         4.0|(50,[0,1,3,4,5,6,...|\n",
      "|       1.0|         4.0|(50,[1,5,7,9,13,1...|\n",
      "|       1.0|         0.0|(50,[0,1,3,5,8,9,...|\n",
      "|       0.0|         0.0|(50,[1,12,13,21,2...|\n",
      "|       0.0|         0.0|(50,[1,4,17,22,24...|\n",
      "|       0.0|         0.0|(50,[1,5,9,10,12,...|\n",
      "|       0.0|         0.0|(50,[1,3,9,10,13,...|\n",
      "|       1.0|         0.0|(50,[0,1,10,17,18...|\n",
      "|       0.0|         3.0|(50,[0,1,3,4,5,6,...|\n",
      "|       2.0|         3.0|(50,[0,1,2,4,6,7,...|\n",
      "|       2.0|         2.0|(50,[1,9,14,17,21...|\n",
      "|       0.0|         2.0|(50,[0,1,9,10,13,...|\n",
      "|       0.0|         2.0|(50,[1,3,7,14,15,...|\n",
      "|       2.0|         2.0|(50,[1,4,5,6,8,9,...|\n",
      "|       0.0|         2.0|(50,[1,3,6,7,8,10...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(rescaledData)\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.01, elasticNetParam=0.5, labelCol=\"indexedLabel\")\n",
    "pipeline = Pipeline(stages=[labelIndexer, lr])\n",
    "\n",
    "#Fit the pipeline to training documents\n",
    "\n",
    "model = pipeline.fit(trainingData)\n",
    "predictions = model.transform(testData)\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test for deployment - Random Streamed Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+--------------------+----------+\n",
      "| label|           tweet_id|          tweet_text|prediction|\n",
      "+------+-------------------+--------------------+----------+\n",
      "|#china|1384110015968014336|Now @MercedesBenz...|  #vaccine|\n",
      "|#china|1384109980769394690|@woonomic I thoug...|  #vaccine|\n",
      "+------+-------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf_model = IDFModel.load('idf_model_prototype_1')\n",
    "with open('index_hashtag_dict.json', 'r') as fp:\n",
    "    index_dict = json.load(fp)\n",
    "    \n",
    "def process(rdd):\n",
    "    df = spark.read.json(rdd)\n",
    "    tokenizer = Tokenizer(inputCol=\"tweet_text\", outputCol=\"words\")\n",
    "    wordsData = tokenizer.transform(df)\n",
    "    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=50)\n",
    "    featurizedData = hashingTF.transform(wordsData)\n",
    "    rescaledData = idf_model.transform(featurizedData)\n",
    "    predictions = model.transform(rescaledData)\n",
    "    predictions = predictions.withColumn(\"prediction\", predictions[\"prediction\"].cast(StringType()))\n",
    "    predictions = predictions.replace(to_replace=index_dict, subset='prediction')\n",
    "    predictions.select(\"label\", \"tweet_id\", \"tweet_text\", \"prediction\").show(50)\n",
    "\n",
    "process('C:/Users/nusret/Desktop/spark/myoutput-1619177010000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+--------------------+\n",
      "| label|           tweet_id|          tweet_text|\n",
      "+------+-------------------+--------------------+\n",
      "|#china|1384110015968014336|Now MercedesBenz ...|\n",
      "|#china|1384109980769394690| woonomic I thoug...|\n",
      "+------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_punct(text):\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub('\\u2588', '', text)\n",
    "    text = text + ' ' + str(text.count('#'))\n",
    "    text = re.sub('#', '', text)\n",
    "    text = re.sub('http.* ', ' ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = re.sub('\\'', '', text)\n",
    "    text = re.sub('\\’', '', text)\n",
    "    text = re.sub('[!\"$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~“”]', ' ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "def process(rdd):\n",
    "    df = spark.read.json(rdd)\n",
    "    udf_myFunction = udf(remove_punct, StringType())\n",
    "    df = df.withColumn(\"tweet_text\", udf_myFunction(\"tweet_text\"))\n",
    "    df.show()\n",
    "process('Desktop/spark/tweets_streamed/tweet-1618832570000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.0': '#vaccine',\n",
       " '1.0': '#covid',\n",
       " '2.0': '#china',\n",
       " '3.0': '#biden',\n",
       " '4.0': '#stopasianhate',\n",
       " '5.0': '#inflation'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
